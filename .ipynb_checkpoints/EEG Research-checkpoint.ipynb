{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "First import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_test = np.load(\"project/X_test.npy\")\n",
    "y_test = np.load(\"project/y_test.npy\")\n",
    "person_train_valid = np.load(\"project/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"project/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"project/y_train_valid.npy\")\n",
    "person_test = np.load(\"project/person_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore EOG data from last 3 of 25 electrodes\n",
    "# Transpose data so that the second dimension is timesteps\n",
    "X_test = X_test[:, :-3, :]\n",
    "X_test = X_test.transpose([0, 2, 1])\n",
    "X_train_valid = X_train_valid[:, :-3, :]\n",
    "X_train_valid = X_train_valid.transpose([0, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Valid data shape: (2115, 1000, 22)\n",
      "Test data shape: (443, 1000, 22)\n",
      "Training/Valid target shape: (2115,)\n",
      "Test target shape: (443,)\n",
      "Person train/valid shape: (2115, 1)\n",
      "Person test shape: (443, 1)\n"
     ]
    }
   ],
   "source": [
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = X_train_valid.shape[0]\n",
    "num_timesteps = X_train_valid.shape[1]\n",
    "num_features = X_train_valid.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn labels into categories, i.e. y values of 0-3\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train_valid)\n",
    "y_train_valid_classes = le.transform(y_train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1593,  463,  901, ..., 1091,  880, 1750])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_idx = np.random.permutation(num_trials)\n",
    "rand_idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create randomized train and validation dataset split\n",
    "split = int(num_trials * 0.80)\n",
    "X_train = X_train_valid[rand_idx][:split]\n",
    "y_train = y_train_valid_classes[rand_idx][:split]\n",
    "person_train = person_train_valid[rand_idx][:split]\n",
    "\n",
    "X_valid = X_train_valid[rand_idx][split:]\n",
    "y_valid = y_train_valid_classes[rand_idx][split:]\n",
    "person_valid = person_train_valid[rand_idx][split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoints/EEG_prediction.ckpt'\n",
    "import keras.callbacks\n",
    "\n",
    "# Create checkpoint callback\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 save_weights_only=True, \n",
    "                                                 monitor='val_loss',\n",
    "                                                 save_best_only=True, \n",
    "                                                 mode='auto',\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(40, dropout=0.2, input_shape=(num_timesteps, num_features), return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(GRU(20, dropout=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "#model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/200\n",
      "1692/1692 [==============================] - 65s 38ms/step - loss: 1.8253 - acc: 0.2488 - val_loss: 1.6835 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.42605\n",
      "Epoch 2/200\n",
      "1692/1692 [==============================] - 58s 35ms/step - loss: 1.6251 - acc: 0.2600 - val_loss: 1.5902 - val_acc: 0.2009\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.42605\n",
      "Epoch 3/200\n",
      "1692/1692 [==============================] - 55s 33ms/step - loss: 1.5573 - acc: 0.2654 - val_loss: 1.5331 - val_acc: 0.2057\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.42605\n",
      "Epoch 4/200\n",
      "1692/1692 [==============================] - 57s 34ms/step - loss: 1.5087 - acc: 0.2636 - val_loss: 1.5015 - val_acc: 0.2009\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.42605\n",
      "Epoch 5/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.4768 - acc: 0.2807 - val_loss: 1.4792 - val_acc: 0.2104\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.42605\n",
      "Epoch 6/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.4494 - acc: 0.2719 - val_loss: 1.4688 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.42605\n",
      "Epoch 7/200\n",
      "1692/1692 [==============================] - 55s 33ms/step - loss: 1.4247 - acc: 0.3008 - val_loss: 1.4552 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.42605\n",
      "Epoch 8/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3998 - acc: 0.3002 - val_loss: 1.4460 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.42605\n",
      "Epoch 9/200\n",
      "1692/1692 [==============================] - 53s 31ms/step - loss: 1.4004 - acc: 0.2961 - val_loss: 1.4471 - val_acc: 0.2317\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.42605\n",
      "Epoch 10/200\n",
      "1692/1692 [==============================] - 53s 31ms/step - loss: 1.4041 - acc: 0.2796 - val_loss: 1.4410 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.42605\n",
      "Epoch 11/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3893 - acc: 0.2937 - val_loss: 1.4306 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.42605\n",
      "Epoch 12/200\n",
      " 256/1692 [===>..........................] - ETA: 43s - loss: 1.3734 - acc: 0.2852"
     ]
    }
   ],
   "source": [
    "# Fit model on training set, validate with validation data\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=200, batch_size=64, verbose=1, callbacks = [cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
