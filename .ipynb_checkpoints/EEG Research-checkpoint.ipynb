{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "First import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_test = np.load(\"project/X_test.npy\")\n",
    "y_test = np.load(\"project/y_test.npy\")\n",
    "person_train_valid = np.load(\"project/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"project/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"project/y_train_valid.npy\")\n",
    "person_test = np.load(\"project/person_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore EOG data from last 3 of 25 electrodes\n",
    "# Transpose data so that the second dimension is timesteps\n",
    "X_test = X_test[:, :-3, :]\n",
    "X_test = X_test.transpose([0, 2, 1])\n",
    "X_train_valid = X_train_valid[:, :-3, :]\n",
    "X_train_valid = X_train_valid.transpose([0, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Valid data shape: (2115, 1000, 22)\n",
      "Test data shape: (443, 1000, 22)\n",
      "Training/Valid target shape: (2115,)\n",
      "Test target shape: (443,)\n",
      "Person train/valid shape: (2115, 1)\n",
      "Person test shape: (443, 1)\n"
     ]
    }
   ],
   "source": [
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = X_train_valid.shape[0]\n",
    "num_timesteps = X_train_valid.shape[1]\n",
    "num_features = X_train_valid.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn labels into categories, i.e. y values of 0-3\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train_valid)\n",
    "y_train_valid_classes = le.transform(y_train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1593,  463,  901, ..., 1091,  880, 1750])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_idx = np.random.permutation(num_trials)\n",
    "rand_idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create randomized train and validation dataset split\n",
    "split = int(num_trials * 0.80)\n",
    "X_train = X_train_valid[rand_idx][:split]\n",
    "y_train = y_train_valid_classes[rand_idx][:split]\n",
    "person_train = person_train_valid[rand_idx][:split]\n",
    "\n",
    "X_valid = X_train_valid[rand_idx][split:]\n",
    "y_valid = y_train_valid_classes[rand_idx][split:]\n",
    "person_valid = person_train_valid[rand_idx][split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoints/EEG_prediction.ckpt'\n",
    "import keras.callbacks\n",
    "\n",
    "# Create checkpoint callback\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 save_weights_only=True, \n",
    "                                                 monitor='val_loss',\n",
    "                                                 save_best_only=True, \n",
    "                                                 mode='auto',\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(40, dropout=0.2, input_shape=(num_timesteps, num_features), return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(GRU(20, dropout=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "#model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/200\n",
      "1692/1692 [==============================] - 62s 37ms/step - loss: 1.8440 - acc: 0.2630 - val_loss: 1.6986 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.69855, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 2/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.7224 - acc: 0.2541 - val_loss: 1.6458 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.69855 to 1.64580, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 3/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.6031 - acc: 0.2595 - val_loss: 1.5699 - val_acc: 0.2553\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.64580 to 1.56987, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 4/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.5852 - acc: 0.2577 - val_loss: 1.5339 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.56987 to 1.53394, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 5/200\n",
      "1692/1692 [==============================] - 55s 33ms/step - loss: 1.5459 - acc: 0.2695 - val_loss: 1.5130 - val_acc: 0.2530\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.53394 to 1.51295, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 6/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.4950 - acc: 0.2642 - val_loss: 1.4971 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.51295 to 1.49711, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 7/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.5242 - acc: 0.2547 - val_loss: 1.4836 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.49711 to 1.48365, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 8/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.4759 - acc: 0.2754 - val_loss: 1.4753 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.48365 to 1.47534, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 9/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.4722 - acc: 0.2796 - val_loss: 1.4677 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.47534 to 1.46773, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 10/200\n",
      "1692/1692 [==============================] - 55s 33ms/step - loss: 1.4569 - acc: 0.2630 - val_loss: 1.4530 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.46773 to 1.45298, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 11/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.4456 - acc: 0.2837 - val_loss: 1.4442 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.45298 to 1.44421, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 12/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.4281 - acc: 0.2801 - val_loss: 1.4348 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.44421 to 1.43480, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 13/200\n",
      "1692/1692 [==============================] - 56s 33ms/step - loss: 1.4162 - acc: 0.2961 - val_loss: 1.4276 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.43480 to 1.42760, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 14/200\n",
      "1692/1692 [==============================] - 56s 33ms/step - loss: 1.4264 - acc: 0.2861 - val_loss: 1.4252 - val_acc: 0.2246\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.42760 to 1.42518, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 15/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.4208 - acc: 0.2766 - val_loss: 1.4202 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.42518 to 1.42023, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 16/200\n",
      "1692/1692 [==============================] - 55s 33ms/step - loss: 1.4018 - acc: 0.2926 - val_loss: 1.4190 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.42023 to 1.41895, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 17/200\n",
      "1692/1692 [==============================] - 57s 33ms/step - loss: 1.4116 - acc: 0.3008 - val_loss: 1.4155 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.41895 to 1.41550, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 18/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.4134 - acc: 0.2630 - val_loss: 1.4163 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.41550\n",
      "Epoch 19/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.4105 - acc: 0.2760 - val_loss: 1.4137 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.41550 to 1.41366, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 20/200\n",
      "1692/1692 [==============================] - 55s 33ms/step - loss: 1.3934 - acc: 0.2937 - val_loss: 1.4134 - val_acc: 0.2506\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.41366 to 1.41336, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 21/200\n",
      "1692/1692 [==============================] - 55s 33ms/step - loss: 1.4005 - acc: 0.2855 - val_loss: 1.4127 - val_acc: 0.2293\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.41336 to 1.41273, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 22/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.3923 - acc: 0.2884 - val_loss: 1.4143 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.41273\n",
      "Epoch 23/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.4035 - acc: 0.2825 - val_loss: 1.4141 - val_acc: 0.2199\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.41273\n",
      "Epoch 24/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.3869 - acc: 0.2955 - val_loss: 1.4134 - val_acc: 0.2293\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.41273\n",
      "Epoch 25/200\n",
      "1692/1692 [==============================] - 55s 33ms/step - loss: 1.3873 - acc: 0.2967 - val_loss: 1.4130 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.41273\n",
      "Epoch 26/200\n",
      "1692/1692 [==============================] - 55s 33ms/step - loss: 1.3823 - acc: 0.2872 - val_loss: 1.4094 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.41273 to 1.40936, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 27/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.3792 - acc: 0.3132 - val_loss: 1.4099 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.40936\n",
      "Epoch 28/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.3872 - acc: 0.2991 - val_loss: 1.4111 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.40936\n",
      "Epoch 29/200\n",
      "1692/1692 [==============================] - 56s 33ms/step - loss: 1.3856 - acc: 0.3103 - val_loss: 1.4106 - val_acc: 0.2482\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.40936\n",
      "Epoch 30/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3817 - acc: 0.2902 - val_loss: 1.4096 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.40936\n",
      "Epoch 31/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3714 - acc: 0.3050 - val_loss: 1.4098 - val_acc: 0.2459\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.40936\n",
      "Epoch 32/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3879 - acc: 0.2902 - val_loss: 1.4080 - val_acc: 0.2530\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.40936 to 1.40800, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 33/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3768 - acc: 0.3067 - val_loss: 1.4073 - val_acc: 0.2577\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.40800 to 1.40732, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 34/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3831 - acc: 0.2967 - val_loss: 1.4064 - val_acc: 0.2506\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.40732 to 1.40637, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 35/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.3825 - acc: 0.3008 - val_loss: 1.4059 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.40637 to 1.40588, saving model to checkpoints/EEG_prediction.ckpt\n",
      "Epoch 36/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.3716 - acc: 0.3156 - val_loss: 1.4065 - val_acc: 0.2459\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.40588\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3773 - acc: 0.3014 - val_loss: 1.4071 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.40588\n",
      "Epoch 38/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3715 - acc: 0.3245 - val_loss: 1.4077 - val_acc: 0.2459\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.40588\n",
      "Epoch 39/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3827 - acc: 0.2902 - val_loss: 1.4082 - val_acc: 0.2482\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.40588\n",
      "Epoch 40/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3663 - acc: 0.3091 - val_loss: 1.4073 - val_acc: 0.2553\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.40588\n",
      "Epoch 41/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3673 - acc: 0.3085 - val_loss: 1.4071 - val_acc: 0.2459\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.40588\n",
      "Epoch 42/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3750 - acc: 0.3126 - val_loss: 1.4080 - val_acc: 0.2270\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.40588\n",
      "Epoch 43/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3613 - acc: 0.3316 - val_loss: 1.4101 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.40588\n",
      "Epoch 44/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3770 - acc: 0.2985 - val_loss: 1.4099 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.40588\n",
      "Epoch 45/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.3677 - acc: 0.3050 - val_loss: 1.4097 - val_acc: 0.2577\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.40588\n",
      "Epoch 46/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3675 - acc: 0.3115 - val_loss: 1.4102 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.40588\n",
      "Epoch 47/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3573 - acc: 0.3221 - val_loss: 1.4114 - val_acc: 0.2482\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.40588\n",
      "Epoch 48/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3712 - acc: 0.3132 - val_loss: 1.4100 - val_acc: 0.2553\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.40588\n",
      "Epoch 49/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3566 - acc: 0.3404 - val_loss: 1.4093 - val_acc: 0.2553\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.40588\n",
      "Epoch 50/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3680 - acc: 0.3221 - val_loss: 1.4087 - val_acc: 0.2600\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.40588\n",
      "Epoch 51/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3611 - acc: 0.3239 - val_loss: 1.4102 - val_acc: 0.2695\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.40588\n",
      "Epoch 52/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3587 - acc: 0.3197 - val_loss: 1.4091 - val_acc: 0.2671\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.40588\n",
      "Epoch 53/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3537 - acc: 0.3428 - val_loss: 1.4103 - val_acc: 0.2671\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.40588\n",
      "Epoch 54/200\n",
      "1692/1692 [==============================] - 53s 31ms/step - loss: 1.3582 - acc: 0.3144 - val_loss: 1.4131 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.40588\n",
      "Epoch 55/200\n",
      "1692/1692 [==============================] - 53s 31ms/step - loss: 1.3569 - acc: 0.3268 - val_loss: 1.4150 - val_acc: 0.2530\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.40588\n",
      "Epoch 56/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3444 - acc: 0.3457 - val_loss: 1.4155 - val_acc: 0.2671\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.40588\n",
      "Epoch 57/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3513 - acc: 0.3416 - val_loss: 1.4178 - val_acc: 0.2553\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.40588\n",
      "Epoch 58/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3478 - acc: 0.3286 - val_loss: 1.4203 - val_acc: 0.2600\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.40588\n",
      "Epoch 59/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.3495 - acc: 0.3310 - val_loss: 1.4252 - val_acc: 0.2695\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.40588\n",
      "Epoch 60/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3517 - acc: 0.3298 - val_loss: 1.4251 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.40588\n",
      "Epoch 61/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3512 - acc: 0.3357 - val_loss: 1.4269 - val_acc: 0.2553\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.40588\n",
      "Epoch 62/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3503 - acc: 0.3404 - val_loss: 1.4269 - val_acc: 0.2506\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.40588\n",
      "Epoch 63/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3464 - acc: 0.3304 - val_loss: 1.4262 - val_acc: 0.2600\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.40588\n",
      "Epoch 64/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3379 - acc: 0.3552 - val_loss: 1.4298 - val_acc: 0.2506\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.40588\n",
      "Epoch 65/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3396 - acc: 0.3457 - val_loss: 1.4283 - val_acc: 0.2530\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.40588\n",
      "Epoch 66/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3408 - acc: 0.3457 - val_loss: 1.4266 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.40588\n",
      "Epoch 67/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3257 - acc: 0.3641 - val_loss: 1.4265 - val_acc: 0.2459\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.40588\n",
      "Epoch 68/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3295 - acc: 0.3688 - val_loss: 1.4309 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.40588\n",
      "Epoch 69/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3343 - acc: 0.3546 - val_loss: 1.4284 - val_acc: 0.2459\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.40588\n",
      "Epoch 70/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3365 - acc: 0.3528 - val_loss: 1.4287 - val_acc: 0.2459\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.40588\n",
      "Epoch 71/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3321 - acc: 0.3446 - val_loss: 1.4333 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.40588\n",
      "Epoch 72/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3311 - acc: 0.3623 - val_loss: 1.4330 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.40588\n",
      "Epoch 73/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3306 - acc: 0.3517 - val_loss: 1.4371 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.40588\n",
      "Epoch 74/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3297 - acc: 0.3587 - val_loss: 1.4417 - val_acc: 0.2459\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.40588\n",
      "Epoch 75/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3110 - acc: 0.3812 - val_loss: 1.4404 - val_acc: 0.2459\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.40588\n",
      "Epoch 76/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3315 - acc: 0.3546 - val_loss: 1.4450 - val_acc: 0.2246\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.40588\n",
      "Epoch 77/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3252 - acc: 0.3499 - val_loss: 1.4522 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.40588\n",
      "Epoch 78/200\n",
      "1692/1692 [==============================] - 53s 31ms/step - loss: 1.3091 - acc: 0.3735 - val_loss: 1.4586 - val_acc: 0.2270\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.40588\n",
      "Epoch 79/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.3123 - acc: 0.3824 - val_loss: 1.4578 - val_acc: 0.2293\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.40588\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3144 - acc: 0.3924 - val_loss: 1.4604 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.40588\n",
      "Epoch 81/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.3033 - acc: 0.3765 - val_loss: 1.4643 - val_acc: 0.2246\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.40588\n",
      "Epoch 82/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3218 - acc: 0.3741 - val_loss: 1.4632 - val_acc: 0.2246\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.40588\n",
      "Epoch 83/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3073 - acc: 0.3859 - val_loss: 1.4541 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.40588\n",
      "Epoch 84/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3025 - acc: 0.3830 - val_loss: 1.4547 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.40588\n",
      "Epoch 85/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2981 - acc: 0.3907 - val_loss: 1.4611 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.40588\n",
      "Epoch 86/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3002 - acc: 0.3848 - val_loss: 1.4568 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.40588\n",
      "Epoch 87/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.3070 - acc: 0.3883 - val_loss: 1.4665 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.40588\n",
      "Epoch 88/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2948 - acc: 0.4007 - val_loss: 1.4679 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.40588\n",
      "Epoch 89/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2928 - acc: 0.3818 - val_loss: 1.4731 - val_acc: 0.2317\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.40588\n",
      "Epoch 90/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2912 - acc: 0.3924 - val_loss: 1.4822 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.40588\n",
      "Epoch 91/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2855 - acc: 0.4072 - val_loss: 1.4892 - val_acc: 0.2317\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.40588\n",
      "Epoch 92/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2895 - acc: 0.3966 - val_loss: 1.4853 - val_acc: 0.2175\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.40588\n",
      "Epoch 93/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.2711 - acc: 0.4149 - val_loss: 1.4866 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.40588\n",
      "Epoch 94/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.2819 - acc: 0.4060 - val_loss: 1.4850 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.40588\n",
      "Epoch 95/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.2725 - acc: 0.4078 - val_loss: 1.4877 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.40588\n",
      "Epoch 96/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2797 - acc: 0.4048 - val_loss: 1.4948 - val_acc: 0.2317\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.40588\n",
      "Epoch 97/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2623 - acc: 0.4226 - val_loss: 1.4976 - val_acc: 0.2199\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.40588\n",
      "Epoch 98/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2546 - acc: 0.4309 - val_loss: 1.4974 - val_acc: 0.2293\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.40588\n",
      "Epoch 99/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2577 - acc: 0.4031 - val_loss: 1.5033 - val_acc: 0.2270\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.40588\n",
      "Epoch 100/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2507 - acc: 0.4155 - val_loss: 1.4976 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.40588\n",
      "Epoch 101/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.2505 - acc: 0.4184 - val_loss: 1.4916 - val_acc: 0.2246\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.40588\n",
      "Epoch 102/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.2597 - acc: 0.4167 - val_loss: 1.4943 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.40588\n",
      "Epoch 103/200\n",
      "1692/1692 [==============================] - 53s 31ms/step - loss: 1.2588 - acc: 0.4102 - val_loss: 1.4983 - val_acc: 0.2246\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.40588\n",
      "Epoch 104/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.2450 - acc: 0.4320 - val_loss: 1.4922 - val_acc: 0.2270\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.40588\n",
      "Epoch 105/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2399 - acc: 0.4279 - val_loss: 1.4932 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.40588\n",
      "Epoch 106/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2381 - acc: 0.4415 - val_loss: 1.5061 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.40588\n",
      "Epoch 107/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2514 - acc: 0.4167 - val_loss: 1.5129 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.40588\n",
      "Epoch 108/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2222 - acc: 0.4344 - val_loss: 1.5177 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.40588\n",
      "Epoch 109/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2481 - acc: 0.4267 - val_loss: 1.5309 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.40588\n",
      "Epoch 110/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2146 - acc: 0.4450 - val_loss: 1.5232 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.40588\n",
      "Epoch 111/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2247 - acc: 0.4421 - val_loss: 1.5289 - val_acc: 0.2270\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.40588\n",
      "Epoch 112/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2236 - acc: 0.4492 - val_loss: 1.5359 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.40588\n",
      "Epoch 113/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2102 - acc: 0.4486 - val_loss: 1.5520 - val_acc: 0.2057\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.40588\n",
      "Epoch 114/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2034 - acc: 0.4580 - val_loss: 1.5539 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.40588\n",
      "Epoch 115/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1951 - acc: 0.4634 - val_loss: 1.5550 - val_acc: 0.2009\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.40588\n",
      "Epoch 116/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1952 - acc: 0.4504 - val_loss: 1.5563 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.40588\n",
      "Epoch 117/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.2036 - acc: 0.4693 - val_loss: 1.5771 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.40588\n",
      "Epoch 118/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.1871 - acc: 0.4616 - val_loss: 1.5757 - val_acc: 0.2175\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.40588\n",
      "Epoch 119/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2236 - acc: 0.4397 - val_loss: 1.5750 - val_acc: 0.2199\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.40588\n",
      "Epoch 120/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1871 - acc: 0.4693 - val_loss: 1.5954 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.40588\n",
      "Epoch 121/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.2010 - acc: 0.4764 - val_loss: 1.6042 - val_acc: 0.1891\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.40588\n",
      "Epoch 122/200\n",
      "1692/1692 [==============================] - 51s 30ms/step - loss: 1.1939 - acc: 0.4498 - val_loss: 1.6010 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.40588\n",
      "Epoch 123/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1811 - acc: 0.4693 - val_loss: 1.6016 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.40588\n",
      "Epoch 124/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1839 - acc: 0.4693 - val_loss: 1.5896 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.40588\n",
      "Epoch 125/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1722 - acc: 0.4770 - val_loss: 1.6002 - val_acc: 0.2293\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.40588\n",
      "Epoch 126/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1832 - acc: 0.4663 - val_loss: 1.6085 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.40588\n",
      "Epoch 127/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1616 - acc: 0.4657 - val_loss: 1.6120 - val_acc: 0.2104\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.40588\n",
      "Epoch 128/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1670 - acc: 0.4923 - val_loss: 1.5947 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.40588\n",
      "Epoch 129/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1613 - acc: 0.4799 - val_loss: 1.6160 - val_acc: 0.2080\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.40588\n",
      "Epoch 130/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1835 - acc: 0.4746 - val_loss: 1.6196 - val_acc: 0.1986\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.40588\n",
      "Epoch 131/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1664 - acc: 0.4805 - val_loss: 1.6059 - val_acc: 0.2199\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.40588\n",
      "Epoch 132/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1506 - acc: 0.4858 - val_loss: 1.6219 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.40588\n",
      "Epoch 133/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.1517 - acc: 0.4941 - val_loss: 1.6145 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.40588\n",
      "Epoch 134/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1705 - acc: 0.4758 - val_loss: 1.6081 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.40588\n",
      "Epoch 135/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1355 - acc: 0.4953 - val_loss: 1.6153 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.40588\n",
      "Epoch 136/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1625 - acc: 0.4840 - val_loss: 1.6223 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.40588\n",
      "Epoch 137/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1364 - acc: 0.4959 - val_loss: 1.6239 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.40588\n",
      "Epoch 138/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1423 - acc: 0.4935 - val_loss: 1.6439 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.40588\n",
      "Epoch 139/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.1217 - acc: 0.5041 - val_loss: 1.6442 - val_acc: 0.2175\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.40588\n",
      "Epoch 140/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1339 - acc: 0.5035 - val_loss: 1.6509 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.40588\n",
      "Epoch 141/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1199 - acc: 0.5089 - val_loss: 1.6579 - val_acc: 0.2033\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.40588\n",
      "Epoch 142/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1165 - acc: 0.4965 - val_loss: 1.6753 - val_acc: 0.2104\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.40588\n",
      "Epoch 143/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1293 - acc: 0.5000 - val_loss: 1.6703 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.40588\n",
      "Epoch 144/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1105 - acc: 0.5130 - val_loss: 1.6790 - val_acc: 0.2033\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 1.40588\n",
      "Epoch 145/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0995 - acc: 0.5059 - val_loss: 1.6878 - val_acc: 0.2033\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.40588\n",
      "Epoch 146/200\n",
      "1692/1692 [==============================] - 55s 32ms/step - loss: 1.1091 - acc: 0.5213 - val_loss: 1.6666 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1.40588\n",
      "Epoch 147/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1168 - acc: 0.5106 - val_loss: 1.6885 - val_acc: 0.2080\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1.40588\n",
      "Epoch 148/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1175 - acc: 0.5089 - val_loss: 1.7046 - val_acc: 0.2270\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.40588\n",
      "Epoch 149/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0883 - acc: 0.5195 - val_loss: 1.7129 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.40588\n",
      "Epoch 150/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1019 - acc: 0.5083 - val_loss: 1.7135 - val_acc: 0.2009\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.40588\n",
      "Epoch 151/200\n",
      "1692/1692 [==============================] - 53s 31ms/step - loss: 1.0965 - acc: 0.5236 - val_loss: 1.7196 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.40588\n",
      "Epoch 152/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1043 - acc: 0.5254 - val_loss: 1.7102 - val_acc: 0.2009\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.40588\n",
      "Epoch 153/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0792 - acc: 0.5301 - val_loss: 1.6901 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.40588\n",
      "Epoch 154/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0752 - acc: 0.5372 - val_loss: 1.7002 - val_acc: 0.2199\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.40588\n",
      "Epoch 155/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0971 - acc: 0.5266 - val_loss: 1.7273 - val_acc: 0.2057\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.40588\n",
      "Epoch 156/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.1041 - acc: 0.5313 - val_loss: 1.7124 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.40588\n",
      "Epoch 157/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0636 - acc: 0.5384 - val_loss: 1.7211 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.40588\n",
      "Epoch 158/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0907 - acc: 0.5177 - val_loss: 1.7098 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.40588\n",
      "Epoch 159/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0692 - acc: 0.5290 - val_loss: 1.7198 - val_acc: 0.2151\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.40588\n",
      "Epoch 160/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.0883 - acc: 0.5260 - val_loss: 1.7110 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.40588\n",
      "Epoch 161/200\n",
      "1692/1692 [==============================] - 53s 31ms/step - loss: 1.0785 - acc: 0.5284 - val_loss: 1.7129 - val_acc: 0.2175\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.40588\n",
      "Epoch 162/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.0642 - acc: 0.5491 - val_loss: 1.7267 - val_acc: 0.2128\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.40588\n",
      "Epoch 163/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0840 - acc: 0.5343 - val_loss: 1.7507 - val_acc: 0.2104\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.40588\n",
      "Epoch 164/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0778 - acc: 0.5514 - val_loss: 1.7485 - val_acc: 0.2246\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.40588\n",
      "Epoch 165/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0729 - acc: 0.5219 - val_loss: 1.7467 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.40588\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0482 - acc: 0.5437 - val_loss: 1.7299 - val_acc: 0.2317\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.40588\n",
      "Epoch 167/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0534 - acc: 0.5550 - val_loss: 1.7255 - val_acc: 0.2293\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.40588\n",
      "Epoch 168/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0570 - acc: 0.5437 - val_loss: 1.7563 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.40588\n",
      "Epoch 169/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0524 - acc: 0.5455 - val_loss: 1.7769 - val_acc: 0.2246\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.40588\n",
      "Epoch 170/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.0369 - acc: 0.5585 - val_loss: 1.7631 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.40588\n",
      "Epoch 171/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0568 - acc: 0.5420 - val_loss: 1.7511 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.40588\n",
      "Epoch 172/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0307 - acc: 0.5697 - val_loss: 1.7660 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.40588\n",
      "Epoch 173/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0349 - acc: 0.5638 - val_loss: 1.7666 - val_acc: 0.2459\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.40588\n",
      "Epoch 174/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0616 - acc: 0.5384 - val_loss: 1.7770 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.40588\n",
      "Epoch 175/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0613 - acc: 0.5390 - val_loss: 1.7512 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.40588\n",
      "Epoch 176/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.0489 - acc: 0.5485 - val_loss: 1.7555 - val_acc: 0.2270\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.40588\n",
      "Epoch 177/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0168 - acc: 0.5597 - val_loss: 1.7916 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.40588\n",
      "Epoch 178/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0402 - acc: 0.5561 - val_loss: 1.8031 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.40588\n",
      "Epoch 179/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.0071 - acc: 0.5762 - val_loss: 1.7695 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.40588\n",
      "Epoch 180/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.0017 - acc: 0.5833 - val_loss: 1.7786 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.40588\n",
      "Epoch 181/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.0465 - acc: 0.5544 - val_loss: 1.8217 - val_acc: 0.2317\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.40588\n",
      "Epoch 182/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.0132 - acc: 0.5644 - val_loss: 1.7815 - val_acc: 0.2293\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.40588\n",
      "Epoch 183/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0149 - acc: 0.5556 - val_loss: 1.8124 - val_acc: 0.2175\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 1.40588\n",
      "Epoch 184/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0170 - acc: 0.5727 - val_loss: 1.7928 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 1.40588\n",
      "Epoch 185/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0053 - acc: 0.5597 - val_loss: 1.7804 - val_acc: 0.2317\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.40588\n",
      "Epoch 186/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0123 - acc: 0.5786 - val_loss: 1.7899 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.40588\n",
      "Epoch 187/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 0.9872 - acc: 0.5792 - val_loss: 1.7945 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.40588\n",
      "Epoch 188/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0229 - acc: 0.5768 - val_loss: 1.7941 - val_acc: 0.2270\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.40588\n",
      "Epoch 189/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 1.0096 - acc: 0.5762 - val_loss: 1.8299 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.40588\n",
      "Epoch 190/200\n",
      "1692/1692 [==============================] - 53s 32ms/step - loss: 0.9993 - acc: 0.5786 - val_loss: 1.7996 - val_acc: 0.2340\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.40588\n",
      "Epoch 191/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 0.9957 - acc: 0.5804 - val_loss: 1.8033 - val_acc: 0.2506\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.40588\n",
      "Epoch 192/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0010 - acc: 0.5668 - val_loss: 1.8263 - val_acc: 0.2482\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.40588\n",
      "Epoch 193/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0088 - acc: 0.5532 - val_loss: 1.8430 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.40588\n",
      "Epoch 194/200\n",
      "1692/1692 [==============================] - 53s 31ms/step - loss: 0.9845 - acc: 0.5674 - val_loss: 1.8527 - val_acc: 0.2317\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.40588\n",
      "Epoch 195/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 0.9936 - acc: 0.5822 - val_loss: 1.8586 - val_acc: 0.2364\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.40588\n",
      "Epoch 196/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 0.9599 - acc: 0.6058 - val_loss: 1.8364 - val_acc: 0.2317\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.40588\n",
      "Epoch 197/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 0.9927 - acc: 0.5810 - val_loss: 1.8462 - val_acc: 0.2506\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.40588\n",
      "Epoch 198/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 0.9685 - acc: 0.5804 - val_loss: 1.8459 - val_acc: 0.2506\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.40588\n",
      "Epoch 199/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 0.9978 - acc: 0.5810 - val_loss: 1.8372 - val_acc: 0.2600\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1.40588\n",
      "Epoch 200/200\n",
      "1692/1692 [==============================] - 54s 32ms/step - loss: 1.0105 - acc: 0.5644 - val_loss: 1.8137 - val_acc: 0.2600\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.40588\n"
     ]
    }
   ],
   "source": [
    "# Fit model on training set, validate with validation data\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=200, batch_size=64, verbose=1, callbacks = [cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
